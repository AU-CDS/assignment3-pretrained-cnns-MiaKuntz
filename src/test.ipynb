{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf tools\n",
    "import tensorflow as tf\n",
    "\n",
    "# image processsing\n",
    "from tensorflow.keras.preprocessing.image import (load_img,\n",
    "                                                  img_to_array,\n",
    "                                                  ImageDataGenerator)\n",
    "# VGG16 model\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input,\n",
    "                                                 decode_predictions,\n",
    "                                                 VGG16)\n",
    "\n",
    "# layers\n",
    "from tensorflow.keras.layers import (Flatten, \n",
    "                                     Dense, \n",
    "                                     Dropout, \n",
    "                                     BatchNormalization)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# generic model object\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# optimizers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path tools\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe_from_json(json_path):\n",
    "    # Load JSON data into a list of dictionaries\n",
    "    data = []\n",
    "    with open(json_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Convert list of dictionaries to a dataframe\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "test_df = make_dataframe_from_json(\"../in/archive/test_data.json\")\n",
    "train_df = make_dataframe_from_json(\"../in/archive/train_data.json\")\n",
    "val_df = make_dataframe_from_json(\"../in/archive/val_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    ")\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "batch_size = 32\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = os.path.join(\"..\", \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/preprocessing/image.py:1137: UserWarning: Found 7500 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/preprocessing/image.py:1137: UserWarning: Found 91166 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/preprocessing/image.py:1137: UserWarning: Found 7500 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# split the data into three categories\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    directory = image_directory,\n",
    "    x_col = \"image_path\",\n",
    "    y_col = \"class_label\",\n",
    "    target_size = target_size,\n",
    "    color_mode = \"rgb\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    directory = image_directory,\n",
    "    x_col = \"image_path\",\n",
    "    y_col = \"class_label\",\n",
    "    target_size = target_size,\n",
    "    color_mode = \"rgb\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42,\n",
    "    subset = \"training\"\n",
    ")\n",
    "\n",
    "val_images = val_generator.flow_from_dataframe(\n",
    "    dataframe = val_df,\n",
    "    directory = image_directory,\n",
    "    x_col = \"image_path\",\n",
    "    y_col = \"class_label\",\n",
    "    target_size = target_size,\n",
    "    color_mode = \"rgb\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation step\n",
    "augment = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(224,224),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "  layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "  layers.experimental.preprocessing.RandomContrast(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, # this removes the final classification network\n",
    "              pooling='avg', # put an average pooling layer in the top instead\n",
    "              input_shape=(32, 32, 3)) # changing input shape to the predefined shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = train_df[\"class_label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model.input\n",
    "x = augment(inputs)\n",
    "\n",
    "x = Dense(128, activation='relu')(model.output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.45)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.45)(x)\n",
    "\n",
    "\n",
    "outputs = Dense(number_of_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39msgd,\n\u001b[1;32m     13\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[39m# fits the model on batches with real-time data augmentation:\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     18\u001b[0m   train_images,\n\u001b[1;32m     19\u001b[0m   steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(train_images),\n\u001b[1;32m     20\u001b[0m   validation_data \u001b[39m=\u001b[39;49m val_images,\n\u001b[1;32m     21\u001b[0m   validation_steps \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(val_images),\n\u001b[1;32m     22\u001b[0m   epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/preprocessing/image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to retrieve element \u001b[39m\u001b[39m{idx}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbut the Sequence \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhas length \u001b[39m\u001b[39m{length}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx\u001b[39m=\u001b[39midx, length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "# define new model\n",
    "model = Model(inputs=model.inputs, \n",
    "              outputs=outputs)\n",
    "\n",
    "# compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "sgd = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(\n",
    "  train_images,\n",
    "  steps_per_epoch = len(train_images),\n",
    "  validation_data = val_images,\n",
    "  validation_steps = len(val_images),\n",
    "  epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "predictions = model.predict(train_images, batch_size=128)\n",
    "print(classification_report(train_images.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=\"class_label\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

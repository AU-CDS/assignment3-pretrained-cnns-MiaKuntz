{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:10:14.252024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# tf tools\n",
    "import tensorflow as tf\n",
    "\n",
    "# image processsing\n",
    "from tensorflow.keras.preprocessing.image import (load_img,\n",
    "                                                  img_to_array,\n",
    "                                                  ImageDataGenerator)\n",
    "# VGG16 model\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input,\n",
    "                                                 decode_predictions,\n",
    "                                                 VGG16)\n",
    "\n",
    "# layers\n",
    "from tensorflow.keras.layers import (Flatten, \n",
    "                                     Dense, \n",
    "                                     Dropout, \n",
    "                                     BatchNormalization)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# generic model object\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# optimizers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path tools\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading json metadata\n",
    "test_df = pd.read_json(os.path.join(\"..\", \"images\", \"metadata\", \"test_data.json\"), lines=True)\n",
    "train_df = pd.read_json(os.path.join(\"..\", \"images\", \"metadata\", \"train_data.json\"), lines=True)\n",
    "val_df = pd.read_json(os.path.join(\"..\", \"images\", \"metadata\", \"val_data.json\"), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random samle\n",
    "#test_df_sample = test_df.sample(n=3000)\n",
    "#train_df_sample = train_df.sample(n=10000)\n",
    "#val_df_sample = test_df.sample(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random samle\n",
    "test_df_sample = test_df.sample(n=75)\n",
    "train_df_sample = train_df.sample(n=250)\n",
    "val_df_sample = test_df.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generating settings\n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                    rotation_range=20,\n",
    "                                    rescale=1/255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "                                rescale=1./255.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting image directory\n",
    "image_directory = os.path.join(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for sizes\n",
    "batch_size = 32\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 validated image filenames belonging to 15 classes.\n",
      "Found 250 validated image filenames belonging to 15 classes.\n",
      "Found 50 validated image filenames belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# generating images\n",
    "test_images = test_datagen.flow_from_dataframe(\n",
    "    dataframe = test_df_sample,\n",
    "    directory = image_directory,\n",
    "    x_col = \"image_path\",\n",
    "    y_col = \"class_label\",\n",
    "    target_size = target_size,\n",
    "    color_mode = \"rgb\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "train_images = train_datagen.flow_from_dataframe(\n",
    "    dataframe = train_df_sample,\n",
    "    directory = image_directory,\n",
    "    x_col = \"image_path\",\n",
    "    y_col = \"class_label\",\n",
    "    target_size = target_size,\n",
    "    color_mode = \"rgb\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42,\n",
    "    subset = \"training\"\n",
    ")\n",
    "\n",
    "val_images = train_datagen.flow_from_dataframe(\n",
    "    dataframe = val_df_sample,\n",
    "    directory = image_directory,\n",
    "    x_col = \"image_path\",\n",
    "    y_col = \"class_label\",\n",
    "    target_size = target_size,\n",
    "    color_mode = \"rgb\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, # this removes the final classification network\n",
    "              pooling='avg', # put an average pooling layer in the top instead\n",
    "              input_shape=(224, 224, 3)) # changing input shape to the predefined shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark loaded layers as not trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "bn = BatchNormalization()(flat1)\n",
    "class1 = Dense(256, \n",
    "               activation='relu')(bn)\n",
    "class2 = Dense(128, \n",
    "               activation='relu')(class1)\n",
    "output = Dense(15, \n",
    "               activation='softmax')(class2)\n",
    "\n",
    "# define new model\n",
    "model = Model(inputs=model.inputs, \n",
    "              outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                1935      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,882,895\n",
      "Trainable params: 167,183\n",
      "Non-trainable params: 14,715,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "sgd = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:18:41.955227: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    steps_per_epoch = len(train_images),\n",
    "    validation_data = val_images,\n",
    "    validation_steps = len(val_images),\n",
    "    epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting helper function\n",
    "def plot_history(history, epochs):\n",
    "    plt.style.use(\"seaborn-colorblind\")\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\", linestyle=\":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\", linestyle=\":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_history(history, \u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(history, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbs3!!!!!\n",
    "    # save the plot\n",
    "    output_path = os.path.join(\"..\", \"out\", filename)\n",
    "    plt.savefig(output_path)\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 12:03:43.120732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 176s 33s/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameIterator' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# classification report\u001b[39;00m\n\u001b[1;32m      2\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_images, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m report \u001b[39m=\u001b[39m classification_report(\u001b[39mlen\u001b[39m(test_images\u001b[39m.\u001b[39;49margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m      4\u001b[0m                             predictions\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m      5\u001b[0m                             target_names\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclass_label\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameIterator' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "predictions = model.predict(test_images, batch_size=128)\n",
    "report = classification_report(test_images.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=\"class_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining outpath\n",
    "folderpath = os.path.join(\"out\")\n",
    "# defining filename\n",
    "filename = \"classification_report.txt\"\n",
    "# writing and saving classification report\n",
    "filepath = os.path.join(folderpath, filename)\n",
    "with open(filepath, \"w\") as f:\n",
    "    f.write(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
